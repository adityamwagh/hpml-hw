Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 32
Effective Batch Size: 32
Epoch: 1 | Training Loss:  1.831 | Training Accuracy:  32.756
Epoch: 2 | Training Loss:  1.322 | Training Accuracy:  51.776
Finished training using SGD optimizer.
Training Time: 20.793332966044545(s)
Total Time: 22.28627776913345(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 128
Effective Batch Size: 128
Epoch: 1 | Training Loss:  2.008 | Training Accuracy:  28.560
Epoch: 2 | Training Loss:  1.527 | Training Accuracy:  43.506
Finished training using SGD optimizer.
Training Time: 5.673803454264998(s)
Total Time: 6.253277723677456(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 512
Effective Batch Size: 512
Epoch: 1 | Training Loss:  2.158 | Training Accuracy:  26.086
Epoch: 2 | Training Loss:  1.564 | Training Accuracy:  41.046
Finished training using SGD optimizer.
Training Time: 1.5959415901452303(s)
Total Time: 2.0512326043099165(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 2048
Effective Batch Size: 2048
Epoch: 1 | Training Loss:  3.597 | Training Accuracy:  12.378
Epoch: 2 | Training Loss:  2.198 | Training Accuracy:  14.980
Finished training using SGD optimizer.
Training Time: 0.5623538987711072(s)
Total Time: 1.4627770157530904(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 8192
Effective Batch Size: 8192
Traceback (most recent call last):
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 193, in <module>
    main()
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 148, in main
    outputs = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 70, in forward
    out = self.layer2(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 42, in forward
    out = F.relu(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 31.75 GiB total capacity; 29.14 GiB already allocated; 527.75 MiB free; 29.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 32
Effective Batch Size: 64
Epoch: 1 | Training Loss:  1.859 | Training Accuracy:  32.296
Epoch: 2 | Training Loss:  1.365 | Training Accuracy:  50.344
Finished training using SGD optimizer.
Training Time: 17.218268529511988(s)
Total Time: 18.171845637261868(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 128
Effective Batch Size: 256
Epoch: 1 | Training Loss:  1.978 | Training Accuracy:  30.670
Epoch: 2 | Training Loss:  1.411 | Training Accuracy:  47.800
Finished training using SGD optimizer.
Training Time: 4.611002124845982(s)
Total Time: 5.124321402050555(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 512
Effective Batch Size: 1024
Epoch: 1 | Training Loss:  2.021 | Training Accuracy:  27.986
Epoch: 2 | Training Loss:  1.475 | Training Accuracy:  44.036
Finished training using SGD optimizer.
Training Time: 1.3563790880143642(s)
Total Time: 1.999008847400546(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 2048
Effective Batch Size: 4096
Epoch: 1 | Training Loss:  3.401 | Training Accuracy:  13.876
Epoch: 2 | Training Loss:  2.364 | Training Accuracy:  14.014
Finished training using SGD optimizer.
Training Time: 0.5149777103215456(s)
Total Time: 2.2280573807656765(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 8192
Effective Batch Size: 16384
Traceback (most recent call last):
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 193, in <module>
    main()
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 148, in main
    outputs = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 70, in forward
    out = self.layer2(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 42, in forward
    out = F.relu(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 31.75 GiB total capacity; 29.23 GiB already allocated; 431.75 MiB free; 29.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 32
Effective Batch Size: 128
Epoch: 1 | Training Loss:  2.016 | Training Accuracy:  29.080
Epoch: 2 | Training Loss:  1.496 | Training Accuracy:  44.868
Finished training using SGD optimizer.
Training Time: 11.819965320639312(s)
Total Time: 12.517172047868371(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 128
Effective Batch Size: 512
Epoch: 1 | Training Loss:  2.665 | Training Accuracy:  16.478
Epoch: 2 | Training Loss:  1.815 | Training Accuracy:  30.240
Finished training using SGD optimizer.
Training Time: 3.2763641187921166(s)
Total Time: 6.339659574441612(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 512
Effective Batch Size: 2048
Epoch: 1 | Training Loss:  2.905 | Training Accuracy:  13.780
Epoch: 2 | Training Loss:  2.053 | Training Accuracy:  21.082
Finished training using SGD optimizer.
Training Time: 1.0378871308639646(s)
Total Time: 5.178516738116741(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 2048
Effective Batch Size: 8192
Epoch: 1 | Training Loss:  2.301 | Training Accuracy:  17.896
Epoch: 2 | Training Loss:  2.204 | Training Accuracy:  16.580
Finished training using SGD optimizer.
Training Time: 0.4245785940438509(s)
Total Time: 5.808028065599501(s)
Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 8192
Effective Batch Size: 32768
Traceback (most recent call last):
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 193, in <module>
    main()
  File "/scratch/amw9425/hpml-hw/hw4/lab4.py", line 148, in main
    outputs = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 70, in forward
    out = self.layer2(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/amw9425/hpml-hw/hw4/resnet.py", line 42, in forward
    out = F.relu(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 31.75 GiB total capacity; 29.42 GiB already allocated; 237.75 MiB free; 29.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Files already downloaded and verified
Files already downloaded and verified


Started training using SGD optimizer.
Batch size per GPU: 2048
Effective Batch Size: 8192
Epoch: 1 | Training Loss:  2.925 | Training Accuracy:  14.992
Epoch: 2 | Training Loss:  2.830 | Training Accuracy:  10.900
Epoch: 3 | Training Loss:  2.282 | Training Accuracy:  12.596
Epoch: 4 | Training Loss:  2.035 | Training Accuracy:  17.260
Epoch: 5 | Training Loss:  1.864 | Training Accuracy:  21.424
Finished training using SGD optimizer.
Training Time: 0.4261370347812772(s)
Total Time: 5.933243232779205(s)
